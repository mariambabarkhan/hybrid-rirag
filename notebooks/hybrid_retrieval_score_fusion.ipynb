{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5ydo2eOXd_J"
      },
      "source": [
        "# PART 1 - Information Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3M0yX-IXGLR"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcTsAP4bYiuq",
        "outputId": "81bb53d5-97c3-4a0e-8270-ad6adb77f232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.26.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting groq\n",
            "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pyserini==0.22.1\n",
            "  Downloading pyserini-0.22.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (3.0.11)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (2.2.2)\n",
            "Collecting pyjnius>=1.4.0 (from pyserini==0.22.1)\n",
            "  Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (4.66.6)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (4.46.2)\n",
            "Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (0.2.0)\n",
            "Collecting nmslib>=2.1.1 (from pyserini==0.22.1)\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime>=1.8.1 (from pyserini==0.22.1)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (4.5.0)\n",
            "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (3.7.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pyserini==0.22.1) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Collecting pybind11<2.6.2 (from nmslib>=2.1.1->pyserini==0.22.1)\n",
            "  Using cached pybind11-2.6.1-py2.py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.1.1->pyserini==0.22.1) (5.9.5)\n",
            "Collecting coloredlogs (from onnxruntime>=1.8.1->pyserini==0.22.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini==0.22.1) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini==0.22.1) (4.25.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini==0.22.1) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini==0.22.1) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini==0.22.1) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (0.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini==0.22.1) (3.4.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini==0.22.1) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini==0.22.1) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.1->pyserini==0.22.1) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->pyserini==0.22.1) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini==0.22.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini==0.22.1) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini==0.22.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini==0.22.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.1->pyserini==0.22.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.1->pyserini==0.22.1) (7.0.5)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.8.1->pyserini==0.22.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.1->pyserini==0.22.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini==0.22.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini==0.22.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.1->pyserini==0.22.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini==0.22.1) (0.1.2)\n",
            "Downloading pyserini-0.22.1-py3-none-any.whl (140.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading neo4j-5.26.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.1-cp310-cp310-linux_x86_64.whl size=13578482 sha256=87ea5a7c1bc9d2b9d3ca94465243154bcdffb059da8ba4d8b3b0602e7ec3112a\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/1a/5d/4cc754a5b1a88405cad184b76f823897a63a8d19afcd4b9314\n",
            "Successfully built nmslib\n",
            "Installing collected packages: pyjnius, rank_bm25, pybind11, neo4j, humanfriendly, faiss-cpu, nmslib, coloredlogs, onnxruntime, groq, pyserini\n",
            "Successfully installed coloredlogs-15.0.1 faiss-cpu-1.9.0.post1 groq-0.12.0 humanfriendly-10.0 neo4j-5.26.0 nmslib-2.1.1 onnxruntime-1.20.1 pybind11-2.6.1 pyjnius-1.6.1 pyserini-0.22.1 rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk torch faiss-cpu requests numpy neo4j sentence-transformers groq rank_bm25 pyserini==0.22.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEK8pF0uYlW2",
        "outputId": "7895ffa5-3f88-4da1-c3cf-9f2ba997e2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import nltk\n",
        "import time\n",
        "import faiss\n",
        "import torch\n",
        "import pickle\n",
        "import requests\n",
        "import subprocess\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "from typing import List, Dict\n",
        "from google.colab import files\n",
        "from nltk.corpus import wordnet\n",
        "from neo4j import GraphDatabase\n",
        "from rank_bm25 import BM25Okapi\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n31lwkT6Ynx3"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "NEO_USER = userdata.get('NEO_USER')\n",
        "NEO_PASS = userdata.get('NEO_PASS')\n",
        "NEO_URL = userdata.get('NEO_URL')\n",
        "GROQ_URL = userdata.get('GROQ_URL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt0CXuNzHmX1",
        "outputId": "622debd2-0bf8-4146-8744-88963bec0a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'trec_eval'...\n",
            "remote: Enumerating objects: 1147, done.\u001b[K\n",
            "remote: Counting objects: 100% (332/332), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 1147 (delta 264), reused 277 (delta 226), pack-reused 815 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1147/1147), 764.18 KiB | 7.08 MiB/s, done.\n",
            "Resolving deltas: 100% (769/769), done.\n",
            "gcc -g -I.  -Wall -Wno-macro-redefined -DVERSIONID=\\\"10.0-rc2\\\"  -o trec_eval trec_eval.c formats.c meas_init.c meas_acc.c meas_avg.c meas_print_single.c meas_print_final.c gain_init.c get_qrels.c get_trec_results.c get_prefs.c get_qrels_prefs.c get_qrels_jg.c form_res_rels.c form_res_rels_jg.c form_prefs_counts.c utility_pool.c get_zscores.c convert_zscores.c measures.c  m_map.c m_P.c m_num_q.c m_num_ret.c m_num_rel.c m_num_rel_ret.c m_gm_map.c m_Rprec.c m_recip_rank.c m_bpref.c m_iprec_at_recall.c m_recall.c m_Rprec_mult.c m_utility.c m_11pt_avg.c m_ndcg.c m_ndcg_cut.c m_Rndcg.c m_ndcg_rel.c m_binG.c m_G.c m_rel_P.c m_success.c m_infap.c m_map_cut.c m_gm_bpref.c m_runid.c m_relstring.c m_set_P.c m_set_recall.c m_set_rel_P.c m_set_map.c m_set_F.c m_num_nonrel_judged_ret.c m_prefs_num_prefs_poss.c m_prefs_num_prefs_ful.c m_prefs_num_prefs_ful_ret.c m_prefs_simp.c m_prefs_pair.c m_prefs_avgjg.c m_prefs_avgjg_Rnonrel.c m_prefs_simp_ret.c m_prefs_pair_ret.c m_prefs_avgjg_ret.c m_prefs_avgjg_Rnonrel_ret.c m_prefs_simp_imp.c m_prefs_pair_imp.c m_prefs_avgjg_imp.c m_map_avgjg.c m_Rprec_mult_avgjg.c m_P_avgjg.c m_yaap.c m_rbp.c m_rbp_resid.c m_unjudged.c -lm\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/usnistgov/trec_eval.git && cd trec_eval && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LSfjV08HpZF",
        "outputId": "10a1a118-790b-481a-e192-4cb7a5bbcfa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RePASs'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 140 (delta 61), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (140/140), 306.77 KiB | 2.92 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "/content/RePASs\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.46.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.66.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.26.4)\n",
            "Collecting tiktoken (from -r requirements.txt (line 10))\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.25.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (3.7.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 5)) (11.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (0.13.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (2.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 13)) (3.4.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 13)) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 13)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 13)) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 13)) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 13)) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 13)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 13)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 13)) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 13)) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 13)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 13)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 13)) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 13)) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 13)) (0.1.2)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RegNLP/RePASs.git\n",
        "%cd RePASs\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-a0MbywJHzCL",
        "outputId": "cdfcdee0-be65-4f9c-cbab-4c9ea35f88bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/RePASs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/RePASs/models/obligation-classifier-legalbert', exist_ok=True)"
      ],
      "metadata": {
        "id": "TOO6tOYUVXuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWUXEvT1H4O5",
        "outputId": "6893a88b-2d47-4667-da02-d5286ab30e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEQEHmZvXKxT"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FvxoL9eY48r",
        "outputId": "19275032-46fc-4c78-813f-6dfad8bdaa60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ObliQADataset'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 80 (delta 17), reused 47 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (80/80), 11.87 MiB | 14.17 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RegNLP/ObliQADataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SOoFGR7elzV"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/ObliQADataset/ObliQA_test.json\", \"r\") as f:\n",
        "    test_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm-Vyt6deozt"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/ObliQADataset/ObliQA_train.json\", \"r\") as f:\n",
        "    train_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIY2XNaYnBBG"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/ObliQADataset/RIRAGSharedTask/RIRAG_Unseen_Questions.json\", \"r\") as f:\n",
        "    unseen_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c0MLelhV2Di"
      },
      "outputs": [],
      "source": [
        "data = train_data + test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-cDtXIyXN0I"
      },
      "source": [
        "## Generate Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ2wyjUTX-vX"
      },
      "source": [
        "### LegalBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUFwI4XpfJ7K"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/RePASs/models/obligation-classifier-legalbert\"\n",
        "\n",
        "legal_bert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "legal_bert_model = AutoModel.from_pretrained(model_path, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5yk-IPJsn_Z"
      },
      "outputs": [],
      "source": [
        "def embed_text(text: str):\n",
        "    inputs = legal_bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = legal_bert_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57h6FiCnsP6H"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "questions = []\n",
        "bm25_corpus = []\n",
        "\n",
        "for item in data:\n",
        "    questions.append({\"QuestionID\": item[\"QuestionID\"], \"Question\": item[\"Question\"]})\n",
        "    for passage in item[\"Passages\"]:\n",
        "        corpus.append({\"Passage\": passage[\"Passage\"], \"DocumentID\": passage[\"DocumentID\"], \"PassageID\": passage[\"PassageID\"]})\n",
        "        bm25_corpus.append(passage[\"Passage\"])\n",
        "\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in bm25_corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8AaxLW4r1cW",
        "outputId": "b32362c5-938b-4b8b-d0d6-243506676703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (25081, 768)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    all_question_embeddings = np.load(\"/content/all_question_embeddings.npy\")\n",
        "    print(\"Embeddings shape:\", all_question_embeddings.shape)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading embeddings: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHLNZub_rG4b",
        "outputId": "74940b38-c957-45a4-9b77-f19ffc181a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    index_file_path = \"/content/faiss_index.index\"\n",
        "    index = faiss.read_index(index_file_path)\n",
        "    print(\"FAISS index loaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzl1KdT3sRba",
        "outputId": "6ab338d7-5dca-43e5-95cb-ee5ca481d10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: [Errno 2] No such file or directory: '/content/question_to_id_map.pkl'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  with open(\"/content/question_to_id_map.pkl\", \"rb\") as f:\n",
        "    question_to_id_map = pickle.load(f)\n",
        "    print(\"Question to ID map loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmysWzwyXQ7B"
      },
      "source": [
        "## Set up Graph DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JCcwJIjTSMo"
      },
      "outputs": [],
      "source": [
        "driver = GraphDatabase.driver(NEO_URL, auth=(NEO_USER, NEO_PASS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB6XTufSapjT"
      },
      "outputs": [],
      "source": [
        "def load_data_into_neo4j(data):\n",
        "    with driver.session() as session:\n",
        "        for item in data:\n",
        "            question_id = item[\"QuestionID\"]\n",
        "            question_text = item[\"Question\"]\n",
        "            group_id = item[\"Group\"]\n",
        "\n",
        "            # Create Question node with Group property\n",
        "            session.run(\"\"\"\n",
        "                MERGE (q:Question {QuestionID: $question_id})\n",
        "                ON CREATE SET q.Question = $question_text, q.Group = $group_id\n",
        "            \"\"\", question_id=question_id, question_text=question_text, group_id=group_id)\n",
        "\n",
        "            # Create Group node\n",
        "            session.run(\"\"\"\n",
        "                MERGE (g:Group {GroupID: $group_id})\n",
        "            \"\"\", group_id=group_id)\n",
        "\n",
        "            # Create relationship between Question and Group\n",
        "            session.run(\"\"\"\n",
        "                MATCH (q:Question {QuestionID: $question_id}), (g:Group {GroupID: $group_id})\n",
        "                MERGE (q)-[:QUESTION_IN_GROUP]->(g)\n",
        "            \"\"\", question_id=question_id, group_id=group_id)\n",
        "\n",
        "            # Create Passage nodes and link to Question\n",
        "            for passage in item[\"Passages\"]:\n",
        "                document_id = passage[\"DocumentID\"]\n",
        "                passage_id = passage[\"PassageID\"]\n",
        "                passage_text = passage[\"Passage\"]\n",
        "\n",
        "                # Create Passage node\n",
        "                session.run(\"\"\"\n",
        "                    MERGE (p:Passage {PassageID: $passage_id, DocumentID: $document_id})\n",
        "                    ON CREATE SET p.Passage = $passage_text\n",
        "                \"\"\", passage_id=passage_id, document_id=document_id, passage_text=passage_text)\n",
        "\n",
        "                # Create relationship between Question and Passage\n",
        "                session.run(\"\"\"\n",
        "                    MATCH (q:Question {QuestionID: $question_id}), (p:Passage {PassageID: $passage_id})\n",
        "                    MERGE (q)-[:QUESTION_HAS_PASSAGE]->(p)\n",
        "                \"\"\", question_id=question_id, passage_id=passage_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCgjV3S0XUMv"
      },
      "source": [
        "## Retrieval Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXg9P4ofbjvE"
      },
      "source": [
        "### Graph Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRuAzcd3bMIO"
      },
      "outputs": [],
      "source": [
        "def get_relevant_graph_passages(question_text, question_id=None, embedder=None, top_k=10):\n",
        "    \"\"\"\n",
        "    Retrieves passages linked to a question from the Neo4j database and returns them\n",
        "    with relevance scores based on their similarity to the query text.\n",
        "\n",
        "    Args:\n",
        "        question_text (str): The text of the question to match passages against.\n",
        "        question_id (str): Optional. The ID of the question.\n",
        "        embedder: A text embedding model for computing embeddings.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: Each dict contains PassageID, DocumentID, Passage, and Score.\n",
        "    \"\"\"\n",
        "    driver = GraphDatabase.driver(NEO_URL, auth=(NEO_USER, NEO_PASS))\n",
        "    query = \"\"\n",
        "    params = {}\n",
        "\n",
        "    if question_id:\n",
        "        query = \"\"\"\n",
        "            MATCH (q:Question {QuestionID: $question_id})-[:QUESTION_HAS_PASSAGE]->(p:Passage)\n",
        "            RETURN p.PassageID AS passage_id, p.DocumentID AS document_id, p.Passage AS passage_text\n",
        "        \"\"\"\n",
        "        params = {\"question_id\": question_id}\n",
        "    elif question_text:\n",
        "        query = \"\"\"\n",
        "            MATCH (q:Question {Question: $question_text})-[:QUESTION_HAS_PASSAGE]->(p:Passage)\n",
        "            RETURN p.PassageID AS passage_id, p.DocumentID AS document_id, p.Passage AS passage_text\n",
        "        \"\"\"\n",
        "        params = {\"question_text\": question_text}\n",
        "    else:\n",
        "        print(\"Provide either question_id or question_text.\")\n",
        "        return []\n",
        "\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, **params)\n",
        "        passages = [{\"PassageID\": record[\"passage_id\"],\n",
        "                     \"DocumentID\": record[\"document_id\"],\n",
        "                     \"Passage\": record[\"passage_text\"]}\n",
        "                    for record in result]\n",
        "\n",
        "    if not passages:\n",
        "        return []\n",
        "\n",
        "    query_embedding = embed_text(question_text).flatten().reshape(1, -1)\n",
        "    passages_embeddings = np.array([embed_text(passage[\"Passage\"]).flatten() for passage in passages])\n",
        "    similarities = cosine_similarity(query_embedding, passages_embeddings)[0]\n",
        "\n",
        "    for idx, passage in enumerate(passages):\n",
        "        passage[\"Score\"] = similarities[idx]\n",
        "\n",
        "    passages = sorted(passages, key=lambda x: x[\"Score\"], reverse=True)\n",
        "\n",
        "    return passages[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UxkB_NT8gP2"
      },
      "outputs": [],
      "source": [
        "query = \"How does the FSRA define and evaluate \\\"principal risks and uncertainties\\\" for a Petroleum Reporting Entity, particularly for the remaining six months of the financial year?\"\n",
        "query_id = \"7073c16e-1974-4051-9064-9f5706c663c7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbzmrzcm8hnI",
        "outputId": "253b67b6-0108-47fc-c1cc-782e4a98b235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PassageID': '10.1.7.(2)', 'DocumentID': 11, 'Passage': 'A Reporting Entity must:\\n(a)\\tprepare such report:\\n(i)\\tfor the first six months of each financial year or period, and if there is a change to the accounting reference date, prepare such report in respect of the period up to the old accounting reference date; and\\n(ii)\\tin accordance with the applicable IFRS standards or other standards acceptable to the Regulator;\\n(b)\\tensure the financial statements have either been audited or reviewed by auditors, and the audit or review by the auditor is included within the report; and\\n(c)\\tensure that the report includes:\\n(i)\\texcept in the case of a Mining Exploration Reporting Entity or a Petroleum Exploration Reporting Entity, an indication of important events that have occurred during the first six months of the financial year, and their impact on the financial statements;\\n(ii)\\texcept in the case of a Mining Exploration Reporting Entity or a Petroleum Exploration Reporting Entity, a description of the principal risks and uncertainties for the remaining six months of the financial year; and\\n(iii)\\ta condensed set of financial statements, an interim management report and associated responsibility statements.', 'Score': 0.5357752}\n"
          ]
        }
      ],
      "source": [
        "for passage in get_relevant_graph_passages(query):\n",
        "    print(passage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WdIFlcYbzUO"
      },
      "source": [
        "### Contextual Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKO82oWqPWng"
      },
      "outputs": [],
      "source": [
        "def get_relevant_vector_passages(query, question_id=None, top_k=10):\n",
        "    query_tokens = word_tokenize(query.lower())\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    top_bm25_indices = np.argsort(bm25_scores)[-top_k:][::-1]\n",
        "    top_bm25_passages = [\n",
        "        {\n",
        "            \"PassageID\": corpus[i][\"PassageID\"],\n",
        "            \"DocumentID\": corpus[i][\"DocumentID\"],\n",
        "            \"Passage\": corpus[i][\"Passage\"],\n",
        "            \"Score\": bm25_scores[i]\n",
        "        }\n",
        "        for i in top_bm25_indices\n",
        "    ]\n",
        "\n",
        "    query_embedding = embed_text(query).flatten().reshape(1, -1)\n",
        "    faiss_distances, faiss_indices = index.search(query_embedding, k=top_k)\n",
        "    top_faiss_passages = [\n",
        "        {\n",
        "            \"PassageID\": corpus[i][\"PassageID\"],\n",
        "            \"DocumentID\": corpus[i][\"DocumentID\"],\n",
        "            \"Passage\": corpus[i][\"Passage\"],\n",
        "            \"Score\": 1 - faiss_distances[0][idx]\n",
        "        }\n",
        "        for idx, i in enumerate(faiss_indices[0])\n",
        "    ]\n",
        "\n",
        "    combined_passages = {}\n",
        "    for passage in top_bm25_passages + top_faiss_passages:\n",
        "        passage_id = passage[\"PassageID\"]\n",
        "        if passage_id not in combined_passages:\n",
        "            combined_passages[passage_id] = passage\n",
        "        else:\n",
        "            combined_passages[passage_id][\"Score\"] = max(\n",
        "                combined_passages[passage_id][\"Score\"], passage[\"Score\"]\n",
        "            )\n",
        "\n",
        "    combined_passages_list = list(combined_passages.values())\n",
        "    combined_embeddings = [embed_text(passage[\"Passage\"]).flatten() for passage in combined_passages_list]\n",
        "\n",
        "    question_embedding = all_question_embeddings[question_to_id_map[question_id]]\n",
        "    similarities = cosine_similarity([question_embedding], combined_embeddings).flatten()\n",
        "\n",
        "    for idx, passage in enumerate(combined_passages_list):\n",
        "        passage[\"Score\"] = similarities[idx]\n",
        "\n",
        "    ranked_passages = sorted(\n",
        "        combined_passages_list,\n",
        "        key=lambda x: x[\"Score\"],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    for rank, passage in enumerate(ranked_passages[:top_k], start=1):\n",
        "        passage[\"Rank\"] = rank\n",
        "\n",
        "    return ranked_passages[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja0mCnTXojC_",
        "outputId": "3183032b-5842-46ad-8d1f-c642ff74ca3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Relevant Passages:\n",
            "{'PassageID': '37)', 'DocumentID': 28, 'Passage': 'BECOMING AWARE OF INSIDE INFORMATION\\nIn considering the operation of Rule 7.2.1,  the concept of ‘awareness’, or knowledge, of Inside Information is central to operation of FSRA’s continuous disclosure framework.  In interpreting Rule 7.2.1, the Listing Authority considers that a Reporting Entity  becomes ‘aware’ of Inside Information if, and as soon as, an Officer  of the Reporting Entity has, or ought reasonably to have, come into possession of the Inside Information in the course of the performance of their duties as an Officer of that Reporting Entity.\\n', 'Score': 0.7444859, 'Rank': 1}\n",
            "{'PassageID': '6.5', 'DocumentID': 39, 'Passage': 'The Regulator will withdraw its permission to use an ADGM mark if a product or service fails to, or chooses not to, maintain the corresponding designation.', 'Score': 0.56410515, 'Rank': 2}\n",
            "{'PassageID': '10.1.7.(2)', 'DocumentID': 11, 'Passage': 'A Reporting Entity must:\\n(a)\\tprepare such report:\\n(i)\\tfor the first six months of each financial year or period, and if there is a change to the accounting reference date, prepare such report in respect of the period up to the old accounting reference date; and\\n(ii)\\tin accordance with the applicable IFRS standards or other standards acceptable to the Regulator;\\n(b)\\tensure the financial statements have either been audited or reviewed by auditors, and the audit or review by the auditor is included within the report; and\\n(c)\\tensure that the report includes:\\n(i)\\texcept in the case of a Mining Exploration Reporting Entity or a Petroleum Exploration Reporting Entity, an indication of important events that have occurred during the first six months of the financial year, and their impact on the financial statements;\\n(ii)\\texcept in the case of a Mining Exploration Reporting Entity or a Petroleum Exploration Reporting Entity, a description of the principal risks and uncertainties for the remaining six months of the financial year; and\\n(iii)\\ta condensed set of financial statements, an interim management report and associated responsibility statements.', 'Score': 0.5357753, 'Rank': 3}\n",
            "{'PassageID': '19)', 'DocumentID': 19, 'Passage': 'FEATURES OF THE VIRTUAL ASSET FRAMEWORK\\nRegulated Activities in relation to Virtual Assets\\nCOBS Rule 17.1.3 operates such that ‘Client Investments’ in GEN and ‘Financial Instruments’ in CMC are read to include Virtual Assets. This means that the various Rules using these terms throughout the FSRA Rulebooks are expanded to capture Virtual Assets, including in particular the Rules contained in Chapters 3 and 6 of COBS.\\n', 'Score': 0.49848902, 'Rank': 4}\n",
            "{'PassageID': '92)', 'DocumentID': 33, 'Passage': 'DIGITAL SECURITIES – INTERMEDIARIES\\nIntermediaries conducting a Regulated Activity in relation to Virtual Assets – Extension into Digital Securities\\nVirtual Asset Custodians may apply to the FSRA to be a DSF in order to provide custody of Digital Securities.  Refer to paragraphs 73 to 75 for further information on the requirements that will apply.\\n', 'Score': 0.48864722, 'Rank': 5}\n",
            "{'PassageID': '8.3.2.Guidance on CDD.9.', 'DocumentID': 1, 'Passage': 'When employing an eKYC System to assist with CDD, a Relevant Person should:\\na.\\tensure that it has a thorough understanding of the eKYC System itself and the risks of eKYC, including those outlined by relevant guidance from FATF and other international standard setting bodies;\\nb.\\tcomply with all the Rules of the Regulator relevant to eKYC including, but not limited to, applicable requirements regarding the business risk assessment, as per Rule \\u200e6.1, and outsourcing, as per Rule \\u200e9.3;\\nc.\\tcombine eKYC with transaction monitoring, anti-fraud and cyber-security measures to support a wider framework preventing applicable Financial Crime; and\\nd.\\ttake appropriate steps to identify, assess and mitigate the risk of the eKYC system being misused for the purposes of Financial Crime.', 'Score': 0.4715017, 'Rank': 6}\n",
            "{'PassageID': '8.6.1', 'DocumentID': 1, 'Passage': \"When undertaking ongoing CDD under Rule \\u200e8.3.1(1)(d), a Relevant Person must:\\n(a)\\tmonitor Transactions undertaken during the course of its customer relationship to ensure that the Transactions are consistent with the Relevant Person's knowledge of the customer, his business and risk rating;\\n(b)\\tpay particular attention to any complex or unusually large Transactions or unusual patterns of Transactions that have no apparent or visible economic or legitimate purpose;\\n(c)\\tenquire into the background and purpose of the Transactions in (b);\\n(d)\\tperiodically review the adequacy of the CDD information it holds on customers and Beneficial Owners to ensure that the information is kept up to date, particularly for customers with a high-risk rating; and\\n(e)\\tperiodically review each customer to ensure that the risk rating assigned to a customer under Rule \\u200e7.1.1(1)(b) remains appropriate for the customer in light of the money laundering risks.\", 'Score': 0.44514588, 'Rank': 7}\n",
            "{'PassageID': '9.1.1.(3)', 'DocumentID': 1, 'Passage': 'Where a Relevant Person seeks to rely on a Person in (1) it may only do so if and to the extent that:\\n(a)\\tit immediately obtains the necessary CDD information from the third party in (1);\\n(b)\\tit takes adequate steps to satisfy itself that certified copies of the documents used to undertake the relevant elements of CDD will be available from the third party on request without delay;\\n(c)\\tthe Person in (1)(b) to (d) is subject to regulation, including AML/TFS compliance requirements, by a Non-ADGM Financial Services Regulator or other competent authority in a country with AML/TFS regulations which are equivalent to the standards set out in the FATF Recommendations and it is supervised for compliance with such regulations;\\n(d)\\tthe Person in (1) has not relied on any exception from the requirement to conduct any relevant elements of CDD which the Relevant Person seeks to rely on; and\\n(e)\\tin relation to (2), the information is up to date.', 'Score': 0.43024626, 'Rank': 8}\n",
            "{'PassageID': 'Part 6.Chapter 6.78.(5)', 'DocumentID': 14, 'Passage': 'For the purposes of supporting, or giving full effect to, a Recognised Foreign Resolution Action, the Regulator may exercise one or more Resolution Tools, or one or more Resolution Powers, subject to any requirement for ex-ante judicial approval in compliance with section 27.', 'Score': 0.42857915, 'Rank': 9}\n",
            "{'PassageID': '5.2.7', 'DocumentID': 7, 'Passage': 'Consideration and assessment of applications: In order to become authorised to carry on one or more Regulated Activities, the applicant must demonstrate to the satisfaction of the Regulator that it:\\n(1)\\thas adequate and appropriate resources, including financial resources;\\n(2)\\tis fit and proper;\\n(3)\\tis capable of being effectively supervised; and\\n(4)\\thas adequate compliance arrangements, including policies and procedures, that will enable it to comply with all the applicable legal requirements, including the Rules.\\n\\n', 'Score': 0.379542, 'Rank': 10}\n"
          ]
        }
      ],
      "source": [
        "top_passages = get_relevant_vector_passages(query, query_id)\n",
        "print(\"Top Relevant Passages:\")\n",
        "for passage in top_passages:\n",
        "    print(passage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbmvrNE8jQjb"
      },
      "source": [
        "## Hybrid Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_retrieval(query, question_id=None, top_k=10):\n",
        "    \"\"\"\n",
        "    Hybrid retrieval function that retrieves top graph and vector passages,\n",
        "    recalculates their relevance to the query, and returns the top combined results.\n",
        "    \"\"\"\n",
        "    if not isinstance(query, str):\n",
        "        raise ValueError(f\"Query should be a string, but got {type(query)}\")\n",
        "\n",
        "    graph_results = get_relevant_graph_passages(query, question_id, top_k=10)\n",
        "    top_graph_passages = graph_results\n",
        "\n",
        "    vector_results = get_relevant_vector_passages(query, question_id, top_k=10)\n",
        "    top_vector_passages = vector_results\n",
        "\n",
        "    combined_passages = {p[\"PassageID\"]: p for p in top_graph_passages}\n",
        "    for p in top_vector_passages:\n",
        "        if p[\"PassageID\"] not in combined_passages:\n",
        "            combined_passages[p[\"PassageID\"]] = p\n",
        "        else:\n",
        "            combined_passages[p[\"PassageID\"]][\"Score\"] = max(\n",
        "                combined_passages[p[\"PassageID\"]][\"Score\"], p[\"Score\"]\n",
        "            )\n",
        "\n",
        "    combined_passages_list = list(combined_passages.values())\n",
        "\n",
        "    query_embedding = embed_text(query).flatten().reshape(1, -1)\n",
        "    combined_embeddings = [embed_text(p[\"Passage\"]).flatten() for p in combined_passages_list]\n",
        "    similarities = cosine_similarity(query_embedding, combined_embeddings).flatten()\n",
        "\n",
        "    for idx, passage in enumerate(combined_passages_list):\n",
        "        passage[\"Score\"] = similarities[idx]\n",
        "\n",
        "    ranked_passages = sorted(combined_passages_list, key=lambda x: x[\"Score\"], reverse=True)\n",
        "\n",
        "    top_combined_results = []\n",
        "    for rank, passage in enumerate(ranked_passages[:top_k], start=1):\n",
        "        top_combined_results.append({\n",
        "            \"PassageID\": passage[\"PassageID\"],\n",
        "            \"DocumentID\": passage[\"DocumentID\"],\n",
        "            \"Score\": passage[\"Score\"]*100,\n",
        "            \"Rank\": rank,\n",
        "            \"Passage\": passage[\"Passage\"]\n",
        "        })\n",
        "\n",
        "    return top_combined_results"
      ],
      "metadata": {
        "id": "mHixjcv3_6mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What type of procedures must a Third Party Provider establish and maintain to handle issues such as major operational and security incidents?\"\n",
        "query_id = \"d34e3516-f053-4652-a0ac-ede703144b9a\""
      ],
      "metadata": {
        "id": "UB5k0DXn8U_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VSRGA5MyN56",
        "outputId": "9a2b80b5-7a9b-4c7e-89b6-f2ada3a0a680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Relevant Passages:\n",
            "\n",
            "\n",
            "{'PassageID': '20.14.1.(2)', 'DocumentID': 3, 'Score': 92.97748804092407, 'Rank': 1, 'Passage': 'As part of that framework, the Third Party Provider must establish and maintain effective incident management procedures, including for the detection and classification of major operational and security incidents.'}\n",
            "{'PassageID': '19.23.1.(2)', 'DocumentID': 3, 'Score': 91.89510345458984, 'Rank': 2, 'Passage': 'Management of operational and security risks. As part of that framework, the Payment Service Provider must establish and maintain effective incident management procedures, including for the detection and classification of major operational and security incidents.'}\n",
            "{'PassageID': '35)', 'DocumentID': 21, 'Score': 90.609610080719, 'Rank': 3, 'Passage': 'REGULATORY REQUIREMENTS\\nThird Party Outsourcing\\nIn its assessment of a potential third party service provider, the regulated firm must therefore satisfy itself that the service provider maintains robust processes and procedures regarding the relevant service (including, for example, in relation to the testing and security required in this section on Technology Governance).\\n\\n\\n'}\n",
            "{'PassageID': '19.23.1.(1)', 'DocumentID': 3, 'Score': 90.4073715209961, 'Rank': 4, 'Passage': 'Management of operational and security risks. A Payment Service Provider must establish a framework with appropriate mitigation measures and control mechanisms to manage the operational and security risks relating to the Payment Services it provides.'}\n",
            "{'PassageID': '20.5.2', 'DocumentID': 3, 'Score': 89.07566666603088, 'Rank': 5, 'Passage': 'The records maintained by a Third Party Provider must include:\\n(a)\\twhat Specified Information has been requested by the Customer;\\n(b)\\twhat Specified Information has been accessed, processed and transferred by the Third Party Provider; and\\n(c)\\twhat information about the Customer the Third Party Provider has obtained as part of its customer onboarding process.'}\n",
            "{'PassageID': '9.10.5', 'DocumentID': 3, 'Score': 88.99950981140137, 'Rank': 6, 'Passage': 'In preservation of confidential information, Credit Rating Agency Employees must familiarise themselves with the internal securities trading policies maintained by their employer, and periodically certify their compliance as required by such policies.'}\n",
            "{'PassageID': '20.14.3.(5)', 'DocumentID': 3, 'Score': 88.96721601486206, 'Rank': 7, 'Passage': 'For each attestation provided to the Regulator, the Third Party Provider must provide a report by a qualified independent third party that assesses the adequacy of the personnel, procedural and technical controls put in place by the Third Party Provider or any other parties to whom the Third Party Provider may have outsourced operational functions.'}\n",
            "{'PassageID': '108)', 'DocumentID': 30, 'Score': 88.50412368774414, 'Rank': 8, 'Passage': 'SPECIFIC DISCLOSURE REQUIREMENTS\\nCompetent Persons\\nIf a Mining Reporting Entity is not able to meet these conditions then a subsequent disclosure is to include the statements and consent required by Rule 11.12.1.\\n'}\n",
            "{'PassageID': '20.2.2', 'DocumentID': 3, 'Score': 87.41557002067566, 'Rank': 9, 'Passage': 'Unless otherwise agreed in writing between a Third Party Provider and their Customer, the Governing Contract between the Third Party Provider and their Customers must contain the following information –\\n(a)\\tabout the Third Party Provider:\\n(i)\\tthe name of the Third Party Provider;\\n(ii)\\tthe address and contact details of the Third Party Provider’s office in ADGM;\\n(iii)\\tthe name of the Regulator of the Third Party Provider, and details of the Third Party Provider’s Financial Service Permission;\\n(b)\\tabout the Third Party Services:\\n(i)\\ta description of the main characteristics of the Third Party Services to be provided;\\n(ii)\\tthe information or unique identifier that must be provided by the Customer in order for a Third Party Transaction to occur;\\n(iii)\\tthe form and procedure for giving consent to a Third Party Transaction;\\n(iv)\\tthe time of receipt of a Third Party Transaction;\\n(v)\\tthe maximum time taken for the Third Party Services to be provided; and\\n(vi)\\tany limits for the use of the Third Party Services;\\n(c)\\tabout charges and exchange rates:\\n(i)\\tdetails of all charges payable by the Customer to the Third Party Provider, including those connected to  information which is provided or made available and, where applicable, a breakdown of the amounts of all charges;\\n(ii)\\twhere relevant, details of the exchange rates to be applied or, if Reference Exchange Rates are to be used, the method of calculating the relevant date for determining such Reference Exchange Rates;\\n(iii)\\twhere relevant and if agreed, the application of changes in Reference Exchange Rates and information requirements relating to any such changes;\\n(d)\\tabout communication:\\n(i)\\tthe means of communication agreed between the parties for the transmission of information or notifications including, where relevant, any technical requirements for the Customer’s equipment and software for receipt of the information or notifications;\\n(ii)\\tthe manner in which and frequency with which information under this Chapter is to be provided or made available;\\n(iii)\\twhat information relating to the Customer will be transferred to the Customer’s Financial Institution as part of a Third Party Transaction;\\n(iv)\\tthe Customer’s right to receive the revised terms of the Governing Contract and any other information in accordance with Rule 20.2.5;\\n(e)\\tabout safeguards and corrective measures:\\n(i)\\thow and within what period of time the Customer must notify the Third Party Provider of any unauthorised or incorrectly executed Third Party Transaction;\\n(ii)\\tthe secure procedure by which the Third Party Provider will contact the Customer in the event of suspected or actual fraud or security threats;\\n(iii)\\twhere relevant, the conditions under which the Third Party Provider proposes to reserve the right to stop or prevent a Third Party Transaction from being executed;\\n(iv)\\tthe Customer’s liability under Rule 20.12.6 including details of any limits on such liability;\\n(v)\\tthe Third Party Provider’s liability for unauthorised Third Party Transactions under Rule 20.12.5;\\n(vi)\\tthe conditions for the payment of any refund to the Customer under this Chapter\\n(f)\\tabout changes to and termination of the contractual arrangement:\\n(i)\\twhere relevant, the proposed terms under which the Customer will be deemed to have accepted changes to the Governing Contract in accordance with Rule 20.2.6, unless they notify the Third Party Provider that they do not accept such changes before the proposed date of their entry into force;\\n(ii)\\tthe duration of the Governing Contract;\\n(iii)\\twhere relevant, the right of the Customer to terminate the Governing Contract and any agreements relating to termination in accordance with Rule 20.2.6.\\n(g)\\tabout redress:\\n(i)\\tany contractual clauses on the law applicable to the Governing Contract and the competent courts; and\\n(ii)\\tthe availability of any alternative dispute resolution procedures, if applicable, for the Customer and the methods for having access to them.'}\n",
            "{'PassageID': '166).e)', 'DocumentID': 19, 'Score': 86.26598715782166, 'Rank': 10, 'Passage': 'MTF (using Virtual Assets): using third-party issued fiat tokens as a payment/transaction mechanism:\\n\\ni.\\tIn the context of using third party fiat tokens, the Authorised Person must directly meet the requirements of the Accepted Virtual Assets, Technology Governance and AML/CFT sections of this Guidance.\\n\\nii.\\tFor the related fiat currency custody activities, FSRA preference is to have the MTF utilise a Virtual Asset/Fiat Custodian authorised on the basis of paragraphs 139 - 145 or 166(b) above.\\n\\niii.\\tIn relation to the issuance of the related fiat token, in circumstances where the issuer is not authorised under paragraph 166(a) above, it is expected that the Authorised Person undertake the same due diligence as that it would apply for the purposes of determining Accepted Virtual Assets (focusing on Technology Governance requirements, the seven factors used to determine an Accepted Virtual Asset, and requirements relating to reporting and reconciliation).\\n'}\n"
          ]
        }
      ],
      "source": [
        "top_passages = hybrid_retrieval(query, query_id)\n",
        "print(\"Top Relevant Passages:\\n\\n\")\n",
        "for passage in top_passages:\n",
        "    print(passage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIVG5t3FXWnG"
      },
      "source": [
        "## TREC Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfkoH-7gMLyK"
      },
      "outputs": [],
      "source": [
        "def load_qrels(docs_dir: str, fqrels: str) -> Dict[str, Dict[str, int]]:\n",
        "    ndocs = 40\n",
        "    docs = []\n",
        "    for i in range(1, ndocs + 1):\n",
        "        with open(os.path.join(docs_dir, f\"{i}.json\")) as f:\n",
        "            doc = json.load(f)\n",
        "            docs.append(doc)\n",
        "\n",
        "    did2pid2id: Dict[str, Dict[str, str]] = {}\n",
        "    for doc in docs:\n",
        "        for psg in doc:\n",
        "            did2pid2id.setdefault(psg[\"DocumentID\"], {})\n",
        "            assert psg[\"ID\"] not in did2pid2id[psg[\"DocumentID\"]]\n",
        "            did2pid2id[psg[\"DocumentID\"]].setdefault(psg[\"PassageID\"], psg[\"ID\"])\n",
        "\n",
        "    with open(fqrels) as f:\n",
        "        data = json.load(f)\n",
        "    qrels = {}\n",
        "    for e in data:\n",
        "        qid = e[\"QuestionID\"]\n",
        "        for psg in e[\"Passages\"]:\n",
        "            qrels.setdefault(qid, {})\n",
        "            pid = did2pid2id[psg[\"DocumentID\"]][psg[\"PassageID\"]]\n",
        "            qrels[qid][pid] = 1\n",
        "    return did2pid2id, qrels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3AguLV6MORd"
      },
      "outputs": [],
      "source": [
        "did2pid2id, qrels = load_qrels(\"/content/ObliQADataset/StructuredRegulatoryDocuments\", \"/content/ObliQADataset/ObliQA_test.json\")\n",
        "with open(\"qrels\", \"w\") as f:\n",
        "    for qid, rels in qrels.items():\n",
        "        for pid, rel in rels.items():\n",
        "            line = f\"{qid} Q0 {pid} {rel}\"\n",
        "            f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsUx2BYPB9QA"
      },
      "outputs": [],
      "source": [
        "with open('rankings.trec','w') as f:\n",
        "  for question in test_data[1588:]:\n",
        "    query=question['Question']\n",
        "    query_id=question['QuestionID']\n",
        "    retrieved_docs=hybrid_retrieval(query, query_id)\n",
        "    for doc_index, doc in enumerate(retrieved_docs,start=1):\n",
        "      doc_id=doc['DocumentID']\n",
        "      passage_id=doc['PassageID']\n",
        "      pid = did2pid2id.get(doc_id, {}).get(passage_id, passage_id)\n",
        "      if not pid:\n",
        "        print(f\"No matching PID found for doc_id: {doc_id}, passage_id: {passage_id}\")\n",
        "        continue\n",
        "      passage=doc['Passage']\n",
        "      score=doc['Score']\n",
        "      f.write(f\"{query_id} 0 {pid} {doc_index} {score} HybridRetreival\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXOWnT9snKhe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "85717bf3-9720-4277-877e-1a45fbce3664"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2dec3afe-e5ab-4562-a40d-3532411978ac\", \"rankings.trec\", 500182)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/rankings.trec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaFSQGbmz46f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afed5122-109d-48ec-97f3-c766fcb02d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall_10             \tall\t0.7938\n",
            "map_cut_10            \tall\t0.7474\n"
          ]
        }
      ],
      "source": [
        "!trec_eval/trec_eval -m recall.10 -m map_cut.10 /content/qrels /content/rankings.trec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rankings File of Unseen Data"
      ],
      "metadata": {
        "id": "I_myicsmUr6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "\n",
        "for item in unseen_data:\n",
        "    questions.append({\"QuestionID\": item[\"QuestionID\"], \"Question\": item[\"Question\"]})\n",
        "\n",
        "dimension = 768\n",
        "unseen_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "unseen_question_to_id_map = {}\n",
        "unseen_question_embeddings = []\n",
        "\n",
        "for i, question in enumerate(questions):\n",
        "    question_id = question[\"QuestionID\"]\n",
        "    question_text = question[\"Question\"]\n",
        "    question_embedding = embed_text(question_text).flatten()\n",
        "    unseen_question_to_id_map[question_id] = i\n",
        "    unseen_question_embeddings.append(question_embedding)\n",
        "\n",
        "unseen_question_embeddings = np.vstack(unseen_question_embeddings)\n",
        "unseen_index.add(unseen_question_embeddings)\n",
        "\n",
        "faiss.write_index(unseen_index, \"/content/unseen_index.index\")\n",
        "\n",
        "np.save(\"/content/unseen_question_embeddings.npy\", unseen_question_embeddings)\n",
        "\n",
        "with open(\"/content/unseen_question_to_id_map.pkl\", \"wb\") as f:\n",
        "    pickle.dump(unseen_question_to_id_map, f)"
      ],
      "metadata": {
        "id": "t_LpW_hCzwlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_embeddings = np.vstack((all_question_embeddings, unseen_question_embeddings))\n",
        "\n",
        "with open(\"/content/unseen_question_to_id_map.pkl\", \"rb\") as f:\n",
        "    new_question_to_id_map = pickle.load(f)\n",
        "\n",
        "offset = len(question_to_id_map)\n",
        "for question_id, index in new_question_to_id_map.items():\n",
        "    new_question_to_id_map[question_id] = index + offset\n",
        "\n",
        "combined_question_to_id_map = {**question_to_id_map, **new_question_to_id_map}\n",
        "\n",
        "new_index = faiss.read_index(\"/content/unseen_index.index\")\n",
        "\n",
        "index = faiss.read_index(\"/content/faiss_index.index\")\n",
        "\n",
        "index.add(unseen_question_embeddings)\n",
        "\n",
        "faiss.write_index(index, \"/content/updated_index.index\")\n",
        "\n",
        "np.save(\"/content/updated_question_embeddings.npy\", combined_embeddings)\n",
        "\n",
        "with open(\"/content/updated_question_to_id_map.pkl\", \"wb\") as f:\n",
        "    pickle.dump(combined_question_to_id_map, f)"
      ],
      "metadata": {
        "id": "_v2VBw2SU6m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_question_embeddings = np.load(\"/content/updated_question_embeddings.npy\")\n",
        "print(\"Embeddings shape:\", all_question_embeddings.shape)\n",
        "\n",
        "index_file_path = \"/content/updated_index.index\"\n",
        "index = faiss.read_index(index_file_path)\n",
        "print(\"FAISS index loaded successfully.\")\n",
        "\n",
        "with open(\"/content/updated_question_to_id_map.pkl\", \"rb\") as f:\n",
        "    question_to_id_map = pickle.load(f)"
      ],
      "metadata": {
        "id": "TGU6s07gUCi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1efc0477-1b3a-41ea-c153-ae095eb5f874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (25527, 768)\n",
            "FAISS index loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('rankings_unseen.trec','w') as f:\n",
        "  for question in unseen_data:\n",
        "    query=question['Question']\n",
        "    query_id=question['QuestionID']\n",
        "    retrieved_docs=hybrid_retrieval(query, query_id)\n",
        "    for doc_index,doc in enumerate(retrieved_docs,start=1):\n",
        "      doc_id=doc['DocumentID']\n",
        "      passage_id=doc['PassageID']\n",
        "      pid = did2pid2id.get(doc_id, {}).get(passage_id, passage_id)\n",
        "      passage=doc['Passage']\n",
        "      score=doc['Score']\n",
        "      f.write(f\"{query_id} 0 {pid} {doc_index} {score} hybret\\n\")"
      ],
      "metadata": {
        "id": "9Lk-FtTrUEg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/rankings_unseen.trec')"
      ],
      "metadata": {
        "id": "f0MQ70wJVCzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OikZe5mBXmST"
      },
      "source": [
        "# PART 2 - Answer Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqI86bHqXqaZ"
      },
      "source": [
        "## Using llama3-70b-8192 and llama-8b-8192 alternatively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAZEHl0znJfH"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query, context):\n",
        "        prompt = (\n",
        "            \"Based on the following context, provide a detailed and structured answer that directly addresses the question. Each answer sentence must align with a sentence in the source passage(s), covering all critical regulatory obligations and avoiding any contradictions.\\n\\n\"\n",
        "            f\"Question: {query}\\n\"\n",
        "            f\"Context: {context}\\n\"\n",
        "            \"Answer: Provide a comprehensive response that reflects all key requirements and procedures mentioned in the regulatory documents, ensuring factual consistency and alignment with the context.\"\n",
        "        )\n",
        "\n",
        "        response = requests.post(\n",
        "            GROQ_URL,\n",
        "            headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\", \"Content-Type\": \"application/json\"},\n",
        "            json={\n",
        "                \"model\": \"llama3-70b-8192\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['choices'][0]['message']['content']\n",
        "        else:\n",
        "            print(f\"Error: {response.text}\")\n",
        "            return f\"Error: {response.text}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm6VK84kp2Vq"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(query, query_id):\n",
        "    retrieved_docs = hybrid_retrieval(query, query_id)\n",
        "\n",
        "    passages = [doc['Passage'] for doc in retrieved_docs]\n",
        "\n",
        "    context_text = \" \".join(passages)\n",
        "\n",
        "    answer = generate_answer(query, context_text)\n",
        "\n",
        "    result = {\n",
        "        \"QuestionID\": query_id,\n",
        "        \"Question\": query,\n",
        "        \"RetrievedPassage(s)\": passages,\n",
        "        \"Answer\": answer\n",
        "    }\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfe4w6GNqIbc"
      },
      "outputs": [],
      "source": [
        "#Test Run\n",
        "result = run_pipeline(query, query_id)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Unseen Run\n",
        "unseen_id=\"07ced741-8abc-43a2-80b0-7740685481f4\"\n",
        "unseen_query=\"Can the ADGM provide clarification on the processes and procedures a Reporting Entity should follow if it disagrees with the Listing Authority\\u2019s assessment that disclosure is necessary to correct or prevent a false market?\""
      ],
      "metadata": {
        "id": "uTZRtcMYhS8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = run_pipeline(unseen_query, unseen_id)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "w5nd-uUlhYeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLTplWNnrjcV"
      },
      "outputs": [],
      "source": [
        "def load_existing_results(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwf4tX5tZono"
      },
      "outputs": [],
      "source": [
        "results = load_existing_results(\"/content/generated_answers_unseen.json\")\n",
        "\n",
        "test_questions = []\n",
        "\n",
        "for item in unseen_data:\n",
        "    test_questions.append({\"QuestionID\": item[\"QuestionID\"], \"Question\": item[\"Question\"]})\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "for i in range(0, len(test_questions), batch_size):\n",
        "    batch = test_questions[i:i + batch_size]\n",
        "    batch_results = []\n",
        "\n",
        "    for q in batch:\n",
        "        result = run_pipeline(q[\"Question\"], q[\"QuestionID\"])\n",
        "        batch_results.append(result)\n",
        "        time.sleep(10)\n",
        "\n",
        "    results.extend(batch_results)\n",
        "\n",
        "    with open(\"/content/generated_answers_unseen.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(f\"Processed batch {i // batch_size + 1} with {len(batch)} questions.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/generated_answers_unseen.json')"
      ],
      "metadata": {
        "id": "Lp5iziGpUuT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSk5ai3CX12e"
      },
      "source": [
        "### Repass on Llama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RePASs"
      ],
      "metadata": {
        "id": "LWoeRWLVZz4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of Answers Generated for Unseen Questions"
      ],
      "metadata": {
        "id": "NVBRxP1izNz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx0ttsleZbxs"
      },
      "outputs": [],
      "source": [
        "!python /content/RePASs/scripts/evaluate_model.py --input_file /content/generated_answers_unseen.json --group_method_name my_method"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}